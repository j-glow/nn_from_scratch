{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np. random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Neuron class = calculation of: output, activation potential, activation functions\n",
    "class activation_fcn(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # Claculate neuron output\n",
    "    def output(self, layer, name, derivative=False):\n",
    "        # Get the method from 'self'. Default to a lambda.\n",
    "        method = getattr(self, str(name), \"Invalid_function\")\n",
    "        # Call the method as we return it\n",
    "        return method(layer, derivative)\n",
    "\n",
    "    # Display error if wrong activation function name is selected\n",
    "    def Invalid_function(self, *arg):\n",
    "        print(\"Error: Invalid activation function\")\n",
    "        return None\n",
    "\n",
    "    # Identity activation function\n",
    "    def linear(self, layer, derivative=False):\n",
    "        out = 0\n",
    "        if not derivative:\n",
    "            out = layer['activation_potential']\n",
    "        else:\n",
    "            out = np.ones(shape=np.shape(layer['activation_potential']))\n",
    "        return out\n",
    "\n",
    "    # Logistic (sigmoid) activation function\n",
    "    def logistic(self, layer, derivative=False):\n",
    "        out = 0\n",
    "        if not derivative:\n",
    "            out = 1.0 / (1.0 + np.exp(-layer['activation_potential']))\n",
    "        else:\n",
    "            out = layer['output'] * (1.0 - layer['output'])\n",
    "        return out\n",
    "\n",
    "    # Hyperbolic tangent activation function  \n",
    "    def tanh(self, layer, derivative=False):\n",
    "        out = 0\n",
    "        if not derivative:\n",
    "            out = (np.exp(layer['activation_potential']) - np.exp(-layer['activation_potential'])) / (\n",
    "                    np.exp(layer['activation_potential']) + np.exp(-layer['activation_potential']))\n",
    "        else:\n",
    "            out = 1.0 - np.power(layer['output'], 2)\n",
    "        return out\n",
    "\n",
    "    #  ReLU activation function \n",
    "    def relu(self, layer, derivative=False):\n",
    "        out = 0\n",
    "        if not derivative:\n",
    "            out = np.maximum(0, layer['activation_potential'])\n",
    "        else:\n",
    "            out = layer['activation_potential'] >= 0  \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Loss function class\n",
    "class loss_fcn(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # Loss/error value calculated for all input data sample\n",
    "    def loss(self, loss, expected, outputs, derivative):\n",
    "        # Get the method from 'self'. Default to a lambda.\n",
    "        method = getattr(self, str(loss), lambda: \"Invalid loss function\")\n",
    "        # Call the method as we return it\n",
    "        return method(expected, outputs, derivative)\n",
    "\n",
    "    # Display error if wrong loss function name is selected\n",
    "    def Invalid_function(self, *arg):\n",
    "        print(\"Error: Invalid loss function\")\n",
    "        return None\n",
    "\n",
    "    # Mean Square Error loss function\n",
    "    def mse(self, expected, outputs, derivative=False):\n",
    "        error_value = 0\n",
    "        if not derivative:\n",
    "            error_value = 0.5 * np.power(expected - outputs, 2)\n",
    "        else:\n",
    "            error_value = -(expected - outputs)\n",
    "        return error_value\n",
    "\n",
    "    # Cross-entropy loss function\n",
    "    def binary_cross_entropy(self, expected, outputs, derivative=False):\n",
    "        error_value = 0\n",
    "        if not derivative:\n",
    "            error_value = -expected * np.log(outputs) - (1 - expected) * np.log(1 - outputs)\n",
    "        else:\n",
    "            error_value = -(expected / outputs - (1 - expected) / (1 - outputs))\n",
    "        return error_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a network\n",
    "class Neural_network(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def create_network(self, structure):\n",
    "        self.nnetwork = [structure[0]]\n",
    "        for i in range(1, len(structure)):\n",
    "            new_layer = {\n",
    "                'weights': np.random.randn(structure[i]['units'], structure[i-1]['units'] + structure[i]['bias']),\n",
    "                'bias': structure[i]['bias'],\n",
    "                'activation_function': structure[i]['activation_function'],\n",
    "                'activation_potential': None,\n",
    "                'delta': None,\n",
    "                'output': None}\n",
    "            self.nnetwork.append(new_layer)\n",
    "        return self.nnetwork\n",
    "\n",
    "    # Forward propagate input to a network output\n",
    "    def forward_propagate(self, nnetwork, inputs):\n",
    "        # Network input values from dataset\n",
    "        af = activation_fcn()\n",
    "        inp = inputs.copy()\n",
    "        for i in range(1, len(nnetwork)):\n",
    "            if nnetwork[i]['bias']==True:\n",
    "                inp = np.append(inp, 1)\n",
    "            # Storage of network outputs from present layer of network\n",
    "            nnetwork[i]['activation_potential'] = np.matmul(nnetwork[i]['weights'], inp).flatten()\n",
    "            nnetwork[i]['output'] = af.output(nnetwork[i], nnetwork[i]['activation_function'], derivative=False)\n",
    "            inp = nnetwork[i]['output']\n",
    "        return inp\n",
    "\n",
    "\n",
    "    # Backpropagate error and store it in neuron\n",
    "    def backward_propagate(self, loss_function, nnetwork, expected):\n",
    "        # Error prooagation will start from last layer\n",
    "        af = activation_fcn()\n",
    "        loss = loss_fcn()\n",
    "        N = len(nnetwork)-1\n",
    "        for i in range(N, 0, -1):\n",
    "            # Storage of error values from present layer\n",
    "            errors = []\n",
    "            # Calculation of error values for other layers than last layer \n",
    "            if i<N:\n",
    "                weights = nnetwork[i+1]['weights']\n",
    "                if nnetwork[i+1]['bias']==True:\n",
    "                    weights = weights[:, :-1]\n",
    "                errors = np.matmul(nnetwork[i+1]['delta'], weights)\n",
    "            # Calculation of error values for last layer\n",
    "            else:                \n",
    "                errors = loss.loss(loss_function, expected, nnetwork[-1]['output'], derivative=True)\n",
    "            \n",
    "            nnetwork[i]['delta'] = np.multiply(errors, af.output(nnetwork[i], nnetwork[i]['activation_function'], derivative=True))\n",
    "                \n",
    "\n",
    "    # Update network weights with error\n",
    "    def update_weights(self, nnetwork, inputs, l_rate):\n",
    "        inp = inputs\n",
    "        for i in range(1, len(nnetwork)):           \n",
    "            if nnetwork[i]['bias']==True:\n",
    "                inp = np.append(inp, 1)\n",
    "            nnetwork[i]['weights'] -= l_rate * np.matmul(nnetwork[i]['delta'].reshape(-1,1), inp.reshape(1,-1))\n",
    "            inp = nnetwork[i]['output']\n",
    "            \n",
    "\n",
    "    # Train a network for a fixed number of epochs\n",
    "    def train(self, nnetwork, x_train, y_train, l_rate=0.01, n_epoch=100, loss_function='mse', verbose=1):\n",
    "        for epoch in range(n_epoch):\n",
    "            sum_error = 0\n",
    "            for iter, (x_row, y_row) in enumerate(zip(x_train, y_train)):\n",
    "\n",
    "                self.forward_propagate(nnetwork, x_row)\n",
    "                \n",
    "                loss = loss_fcn()\n",
    "                sum_error = np.sum(loss.loss(loss_function, y_row, nnetwork[-1]['output'], derivative=False))\n",
    "\n",
    "                self.backward_propagate(loss_function, nnetwork, y_row)\n",
    "\n",
    "                self.update_weights(nnetwork, x_row, l_rate)                               \n",
    "\n",
    "            if verbose > 0:\n",
    "                print('>epoch=%d, loss=%.3f' % (epoch + 1, sum_error))\n",
    "        print('Results: epoch=%d, loss=%.3f' % (epoch + 1, sum_error))\n",
    "        return nnetwork\n",
    "\n",
    "    # Calculate network output\n",
    "    def predict(self, nnetwork, inputs):\n",
    "        out = []\n",
    "        for input in inputs:\n",
    "            out.append(self.forward_propagate(nnetwork, input))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_data(n=30):\n",
    "    # Class 1 - samples generation\n",
    "    X1_1 = 1 + 4 * np.random.rand(n, 1)\n",
    "    X1_2 = 1 + 4 * np.random.rand(n, 1)\n",
    "    class1 = np.concatenate((X1_1, X1_2), axis=1)\n",
    "    Y1 = np.ones(n)\n",
    "\n",
    "    # Class 0 - samples generation\n",
    "    X0_1 = 3 + 4 * np.random.rand(n, 1)\n",
    "    X0_2 = 3 + 4 * np.random.rand(n, 1)\n",
    "    class0 = np.concatenate((X0_1, X0_2), axis=1)\n",
    "    Y0 = np.zeros(n)\n",
    "\n",
    "    X = np.concatenate((class1, class0))\n",
    "    Y = np.concatenate((Y1, Y0))\n",
    "\n",
    "    idx0 = [i for i, v in enumerate(Y) if v == 0]\n",
    "    idx1 = [i for i, v in enumerate(Y) if v == 1]\n",
    "\n",
    "    return X, Y, idx0, idx1\n",
    "\n",
    "\n",
    "def test_classification():\n",
    "    # Read data\n",
    "    X, Y, idx0, idx1 = generate_classification_data()\n",
    "\n",
    "    # Create network\n",
    "    model = Neural_network()\n",
    "    structure = [{'type': 'input', 'units': 2},\n",
    "                 {'type': 'dense', 'units': 4, 'activation_function': 'relu', 'bias': True},\n",
    "                 {'type': 'dense', 'units': 4, 'activation_function': 'relu', 'bias': True},\n",
    "                 {'type': 'dense', 'units': 1, 'activation_function': 'logistic', 'bias': True}]\n",
    "\n",
    "    network = model.create_network(structure)\n",
    "\n",
    "    model.train(network, X, Y, 0.0001, 2000, 'binary_cross_entropy', 0)\n",
    "\n",
    "    y = model.predict(network, X)\n",
    "    t = 0\n",
    "\n",
    "    for n, m in zip(y, Y):\n",
    "        t += 1 - np.abs(np.round(np.array(n)) - np.array(m))\n",
    "        print(f\"pred = {n}, pred_round = {np.round(n)}, true = {m}\")\n",
    "\n",
    "    ACC = t / len(X)\n",
    "    print(f\"\\nClassification accuracy = {ACC * 100}%\")\n",
    "\n",
    "    # Plotting decision regions\n",
    "    xx, yy = np.meshgrid(np.arange(0, 8, 0.1),\n",
    "                         np.arange(0, 8, 0.1))\n",
    "\n",
    "    X_vis = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    h = model.predict(network, X_vis)\n",
    "    h = np.array(h) >= 0.5\n",
    "    h = np.reshape(h, (len(xx), len(yy)))\n",
    "\n",
    "    plt.contourf(xx, yy, h, cmap='jet')\n",
    "    plt.scatter(X[idx1, 0], X[idx1, 1], marker='^', c=\"red\", edgecolors=\"white\", label=\"class 1\")\n",
    "    plt.scatter(X[idx0, 0], X[idx0, 1], marker='o', c=\"blue\", edgecolors=\"white\", label=\"class 0\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "generate_classification_data()\n",
    "test_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_regression_data(n=30):\n",
    "    # Generate regression dataset\n",
    "    X = np.linspace(-5, 5, n).reshape(-1, 1)\n",
    "    y = np.sin(2 * X) + np.cos(X) + 5\n",
    "    # simulate noise\n",
    "    data_noise = np.random.normal(0, 0.2, n).reshape(-1, 1)\n",
    "    # Generate training data\n",
    "    Y = y + data_noise\n",
    "\n",
    "    return X.reshape(-1, 1), Y.reshape(-1, 1)\n",
    "\n",
    "\n",
    "def test_regression():\n",
    "    # Read data\n",
    "    X, Y = generate_regression_data()\n",
    "\n",
    "    # Create network\n",
    "    model = Neural_network()\n",
    "    structure = [{'type': 'input', 'units': 1},\n",
    "                 {'type': 'dense', 'units': 8, 'activation_function': 'tanh', 'bias': True},\n",
    "                 {'type': 'dense', 'units': 8, 'activation_function': 'tanh', 'bias': True},\n",
    "                 {'type': 'dense', 'units': 1, 'activation_function': 'linear', 'bias': True}]\n",
    "\n",
    "    network = model.create_network(structure)\n",
    "\n",
    "    model.train(network, X, Y, 0.01, 4000, 'mse', 0)\n",
    "\n",
    "    predicted = model.predict(network, X)\n",
    "    std = np.std(predicted - Y)\n",
    "    print(\"\\nStandard deviation = {}\".format(std))\n",
    "\n",
    "    X_test = np.linspace(-7, 7, 100).reshape(-1, 1)\n",
    "    X_test = np.array(X_test).tolist()\n",
    "    predicted = model.predict(network, X_test)\n",
    "\n",
    "    plt.plot(X, Y, 'r--o', label=\"Training data\")\n",
    "    plt.plot(X_test, predicted, 'b--x', label=\"Predicted\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "generate_regression_data(30)\n",
    "test_regression()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
