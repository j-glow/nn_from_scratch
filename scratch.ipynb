{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np. random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Neuron class = calculation of: output, activation potential, activation functions\n",
    "class activation_fcn(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # Claculate neuron output\n",
    "    def output(self, layer, name, derivative=False):\n",
    "        # Get the method from 'self'. Default to a lambda.\n",
    "        method = getattr(self, name, \"Invalid_function\")\n",
    "        # Call the method as we return it\n",
    "        return method(layer, derivative)\n",
    "\n",
    "    # Display error if wrong activation function name is selected\n",
    "    def Invalid_function(self, *arg):\n",
    "        print(\"Error: Invalid activation function\")\n",
    "        return None\n",
    "\n",
    "    # Identity activation function\n",
    "    def linear(self, layer, derivative=False):\n",
    "        print(layer)\n",
    "        if not derivative:\n",
    "            return layer\n",
    "        else:\n",
    "            return np.ones(shape=np.shape(layer))\n",
    "\n",
    "    def tanh(self, layer, derivative=False):\n",
    "        if not derivative:\n",
    "            return np.tanh(layer)\n",
    "        else:\n",
    "            return 1 - np.tanh(layer)**2\n",
    "\n",
    "    def logistic(self, layer, derivative=False):\n",
    "        out = 0\n",
    "        if not derivative:\n",
    "            out = 1/(1 + np.exp(-layer))\n",
    "        else:\n",
    "            out = layer * (1-layer)\n",
    "        return out\n",
    "\n",
    "    def relu(self, layer, derivative=False):\n",
    "        if not derivative:\n",
    "            return np.maximum(0, layer)\n",
    "        else:\n",
    "            return np.where(layer > 0, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Loss function class\n",
    "class loss_fcn(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # Loss/error value calculated for all input data sample\n",
    "    def loss(self, loss, expected, outputs, derivative):\n",
    "        # Get the method from 'self'. Default to a lambda.\n",
    "        method = getattr(self, str(loss), lambda expected, outputs, derivative: \"Invalid loss function\")\n",
    "        # Call the method as we return it\n",
    "\n",
    "        return method(expected, outputs, derivative)\n",
    "\n",
    "    # Display error if wrong loss function name is selected\n",
    "    def Invalid_function(self, *arg):\n",
    "        print(\"Error: Invalid loss function\")\n",
    "        return None\n",
    "\n",
    "    # Mean Square Error loss function\n",
    "    def mse(self, expected, outputs, derivative=False):\n",
    "        error_value = 0\n",
    "        if not derivative:\n",
    "            error_value = 0.5 * np.power(expected - outputs, 2)\n",
    "        else:\n",
    "            error_value = -(expected - outputs)\n",
    "        return error_value\n",
    "\n",
    "    # Cross Entorpy loss function\n",
    "    def binary_cross_entropy(self, expected, outputs, derivative=False):\n",
    "        error_value = 0\n",
    "        if not derivative:\n",
    "            error_value = -(expected * np.log(outputs) + (1 - expected) * np.log(1 - outputs))\n",
    "        else:\n",
    "            error_value = -(expected / outputs) + (1 - expected) / (1 - outputs)\n",
    "        return error_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a network\n",
    "class Neural_network(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def create_network(self, structure, init_weights):\n",
    "        self.nnetwork = [structure[0]]\n",
    "        for i in range(1, len(structure)):\n",
    "            new_layer = {\n",
    "                'weights': None,\n",
    "                'bias': structure[i]['bias'],\n",
    "                'activation_function': structure[i]['activation_function'],\n",
    "                'activation_potential': None,\n",
    "                'delta': None,\n",
    "                'output': None}\n",
    "            if init_weights == \"ones\":\n",
    "                new_layer['weights'] = np.ones((structure[i]['units']))\n",
    "            elif init_weights == \"zeros\":\n",
    "                new_layer['weights'] = np.zeros((structure[i]['units']))\n",
    "            elif init_weights == \"random\":\n",
    "                new_layer['weights'] =  np.random.randn((structure[i]['units']))\n",
    "            else:\n",
    "                pass\n",
    "            self.nnetwork.append(new_layer)\n",
    "        return self.nnetwork\n",
    "\n",
    "    # Forward propagate input to a network output\n",
    "    def forward_propagate(self, nnetwork, inputs):\n",
    "        nnetwork[0]['output'] = inputs\n",
    "\n",
    "        act_fn = activation_fcn()\n",
    "        for i in range(1, len(nnetwork)):\n",
    "            nnetwork[i]['activation_potential'] = np.sum(np.outer(nnetwork[i-1]['output'], nnetwork[i]['weights']),0) + nnetwork[i]['bias']\n",
    "            nnetwork[i]['output'] = act_fn.output(nnetwork[i]['activation_potential'], nnetwork[i]['activation_function'])\n",
    "\n",
    "        return nnetwork[-1]['output']\n",
    "\n",
    "    # Backpropagate error and store it in neuron\n",
    "    def backward_propagate(self, loss_function, nnetwork, expected):\n",
    "        act_fn = activation_fcn()\n",
    "        loss_fn = loss_fcn()\n",
    "\n",
    "        # Calculate delta for output layer\n",
    "        nnetwork[-1]['delta'] = act_fn.output(nnetwork[-1]['output'], nnetwork[-1]['activation_function'], True) * \\\n",
    "                                loss_fn.loss(loss_function, expected, nnetwork[-1]['output'], True)\n",
    "\n",
    "        # Calculate delta for hidden layers\n",
    "        for i in range(len(nnetwork)-2, 0, -1):\n",
    "            nnetwork[i]['delta'] = np.dot(nnetwork[i+1]['weights'], nnetwork[i+1]['delta']) * \\\n",
    "                                   act_fn.output(nnetwork[i]['output'], nnetwork[i]['activation_function'], True)\n",
    "\n",
    "\n",
    "    # Update network weights with error\n",
    "    def update_weights(self, nnetwork, inputs, l_rate):\n",
    "        for i in range(1, len(nnetwork)):\n",
    "            # Update weights\n",
    "            nnetwork[i]['weights'] -= l_rate * nnetwork[i]['delta'] * nnetwork[i-1]['output']\n",
    "            # Update bias\n",
    "            nnetwork[i]['bias'] -= l_rate * nnetwork[i]['delta']\n",
    "\n",
    "\n",
    "    # Train a network for a fixed number of epochs\n",
    "    def train(self, nnetwork, x_train, y_train, l_rate=0.01, n_epoch=100, loss_function='mse', verbose=1):\n",
    "        for epoch in range(n_epoch):\n",
    "            sum_error = 0\n",
    "            for iter, (x_row, y_row) in enumerate(zip(x_train, y_train)):\n",
    "\n",
    "                self.forward_propagate(nnetwork, x_row)\n",
    "\n",
    "                loss = loss_fcn()\n",
    "                sum_error = np.sum(loss.loss(loss_function, y_row, nnetwork[-1]['output'], derivative=False))\n",
    "\n",
    "                self.backward_propagate(loss_function, nnetwork, y_row)\n",
    "\n",
    "                self.update_weights(nnetwork, x_row, l_rate)\n",
    "\n",
    "            if verbose > 0:\n",
    "                print('>epoch=%d, loss=%.3f' % (epoch + 1, sum_error))\n",
    "        print('Results: epoch=%d, loss=%.3f' % (epoch + 1, sum_error))\n",
    "        return nnetwork\n",
    "\n",
    "    # Calculate network output\n",
    "    def predict(self, nnetwork, inputs):\n",
    "\n",
    "        potentials = []\n",
    "        for i in inputs:\n",
    "            potentials.append(self.forward_propagate(nnetwork, i))\n",
    "\n",
    "        return potentials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_data(n=30):\n",
    "    # Class 1 - samples generation\n",
    "    X1_1 = 1 + 4 * np.random.rand(n, 1)\n",
    "    X1_2 = 1 + 4 * np.random.rand(n, 1)\n",
    "    class1 = np.concatenate((X1_1, X1_2), axis=1)\n",
    "    Y1 = np.ones(n)\n",
    "\n",
    "    # Class 0 - samples generation\n",
    "    X0_1 = 3 + 4 * np.random.rand(n, 1)\n",
    "    X0_2 = 3 + 4 * np.random.rand(n, 1)\n",
    "    class0 = np.concatenate((X0_1, X0_2), axis=1)\n",
    "    Y0 = np.zeros(n)\n",
    "\n",
    "    X = np.concatenate((class1, class0))\n",
    "    Y = np.concatenate((Y1, Y0))\n",
    "\n",
    "    idx0 = [i for i, v in enumerate(Y) if v == 0]\n",
    "    idx1 = [i for i, v in enumerate(Y) if v == 1]\n",
    "\n",
    "    return X, Y, idx0, idx1\n",
    "\n",
    "\n",
    "def test_classification():\n",
    "    # Read data\n",
    "    X, Y, idx0, idx1 = generate_classification_data()\n",
    "\n",
    "    # Create network\n",
    "    model = Neural_network()\n",
    "    structure = [{'type': 'input', 'units': 2},\n",
    "                 {'type': 'dense', 'units': 4, 'activation_function': 'relu', 'bias': True},\n",
    "                 {'type': 'dense', 'units': 4, 'activation_function': 'relu', 'bias': True},\n",
    "                 {'type': 'dense', 'units': 1, 'activation_function': 'logistic', 'bias': True}]\n",
    "\n",
    "    network = model.create_network(structure, \"random\")\n",
    "\n",
    "    model.train(network, X, Y, 0.0001, 2000, 'binary_cross_entropy', 0)\n",
    "\n",
    "    y = model.predict(network, X)\n",
    "    t = 0\n",
    "\n",
    "    for n, m in zip(y, Y):\n",
    "        t += 1 - np.abs(np.round(np.array(n)) - np.array(m))\n",
    "        print(f\"pred = {n}, pred_round = {np.round(n)}, true = {m}\")\n",
    "\n",
    "    ACC = t / len(X)\n",
    "    print(f\"\\nClassification accuracy = {ACC * 100}%\")\n",
    "\n",
    "    # Plotting decision regions\n",
    "    xx, yy = np.meshgrid(np.arange(0, 8, 0.1),\n",
    "                         np.arange(0, 8, 0.1))\n",
    "\n",
    "    X_vis = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    h = model.predict(network, X_vis)\n",
    "    h = np.array(h) >= 0.5\n",
    "    h = np.reshape(h, (len(xx), len(yy)))\n",
    "\n",
    "    plt.contourf(xx, yy, h, cmap='jet')\n",
    "    plt.scatter(X[idx1, 0], X[idx1, 1], marker='^', c=\"red\", edgecolors=\"white\", label=\"class 1\")\n",
    "    plt.scatter(X[idx0, 0], X[idx0, 1], marker='o', c=\"blue\", edgecolors=\"white\", label=\"class 0\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "generate_classification_data()\n",
    "test_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_regression_data(n=30):\n",
    "    # Generate regression dataset\n",
    "    X = np.linspace(-5, 5, n).reshape(-1, 1)\n",
    "    y = np.sin(2 * X) + np.cos(X) + 5\n",
    "    # simulate noise\n",
    "    data_noise = np.random.normal(0, 0.2, n).reshape(-1, 1)\n",
    "    # Generate training data\n",
    "    Y = y + data_noise\n",
    "\n",
    "    return X.reshape(-1, 1), Y.reshape(-1, 1)\n",
    "\n",
    "\n",
    "def test_regression():\n",
    "    # Read data\n",
    "    X, Y = generate_regression_data()\n",
    "\n",
    "    # Create network\n",
    "    model = Neural_network()\n",
    "    structure = [{'type': 'input', 'units': 1},\n",
    "                 {'type': 'dense', 'units': 8, 'activation_function': 'tanh', 'bias': True},\n",
    "                 {'type': 'dense', 'units': 8, 'activation_function': 'tanh', 'bias': True},\n",
    "                 {'type': 'dense', 'units': 1, 'activation_function': 'linear', 'bias': True}]\n",
    "\n",
    "    network = model.create_network(structure, 'random')\n",
    "\n",
    "    model.train(network, X, Y, 0.01, 4000, 'mse', 0)\n",
    "\n",
    "    predicted = model.predict(network, X)\n",
    "    std = np.std(predicted - Y)\n",
    "    print(\"\\nStandard deviation = {}\".format(std))\n",
    "\n",
    "    X_test = np.linspace(-7, 7, 100).reshape(-1, 1)\n",
    "    X_test = np.array(X_test).tolist()\n",
    "    predicted = model.predict(network, X_test)\n",
    "\n",
    "    plt.plot(X, Y, 'r--o', label=\"Training data\")\n",
    "    plt.plot(X_test, predicted, 'b--x', label=\"Predicted\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "generate_regression_data(30)\n",
    "test_regression()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
